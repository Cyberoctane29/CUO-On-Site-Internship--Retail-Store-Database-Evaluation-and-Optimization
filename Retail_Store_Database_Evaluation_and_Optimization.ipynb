{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retail Store Database Evaluation & Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is based on my database and SQL internship, where I focused on evaluating and optimizing a music store database. The primary objectives were to refine the database structure, improve query performance, and ensure data integrity. The database encompasses essential components of a music retail system, including customer details, inventory information, and transaction records. Throughout the project, I applied theoretical concepts of database management to a real-world setting, working extensively with MySQL and PostgreSQL environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation and Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary motivation behind this project was to align the database with modern industry standards, ensuring that the music store operates efficiently. This involved a comprehensive review of the database schema, including an assessment of the data types, relationships, and constraints between entities. The goal was to improve query performance through optimization techniques such as indexing, and enhance data quality and security for future scalability.\n",
    "\n",
    "During the internship, I developed critical skills in query optimization, indexing strategies, data quality analysis, and security measures. Each of these areas contributed to my understanding of database management and optimization in a business context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Database Schema Review:**\n",
    "\n",
    "  * I started by analyzing the architecture of the tables and their relationships. This involved checking for the appropriate use of primary and foreign keys to ensure data integrity.\n",
    "  \n",
    "  * The relationships between different entities, such as customers, invoices, tracks, and artists, were verified and evaluated for performance improvements.\n",
    "  \n",
    "* **Data Type and Constraint Assessment:**\n",
    "\n",
    "  * A detailed review of the data types for each column was conducted to ensure that the database design was optimized for storage and retrieval efficiency.\n",
    "  \n",
    "  * Constraints such as primary keys and foreign keys were reviewed to ensure that data remained reliable and accurate across the database.\n",
    "\n",
    "* **Query Optimization:**\n",
    "\n",
    "  * I examined the existing SQL queries and identified areas where query performance could be improved.\n",
    "  \n",
    "  * Techniques such as indexing and query rewriting were applied to optimize the execution plans, leading to faster and more efficient queries.\n",
    "  \n",
    "* **Documentation and Best Practices:**\n",
    "\n",
    "  * As part of the project, I also documented the schema and the query optimizations I performed. This documentation serves as a reference for future developers and database administrators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools and Technologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PostgreSQL: The primary DBMS used to evaluate and optimize the music store database\n",
    "\n",
    "* MySQL: Explored for educational purposes, to understand the differences in syntax and features between MySQL and PostgreSQL.\n",
    "\n",
    "* pgAdmin4: Utilized to monitor query performance, manage the database, and conduct detailed optimizations.\n",
    "\n",
    "* DuckDB: Chosen for the analysis and querying of CSV files uploaded in this Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Redirect stderr to null\n",
    "stderr_fileno = sys.stderr\n",
    "sys.stderr = open(os.devnull, 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Queries for Data Evaluation & Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installed `duckDB` to enable high-performance SQL processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: duckdb in c:\\users\\saswa\\documents\\github\\cuo-on-site-internship--music-store-database-evaluation-and-optimization\\venv\\lib\\site-packages (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DuckDB is an in-process OLAP (Online Analytical Processing) database optimized for analytical queries. Its integration with pandas DataFrames allows for efficient execution of complex SQL queries within this Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stderr.close()\n",
    "sys.stderr = stderr_fileno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imported the necessary libraries for data manipulation and SQL processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T12:38:30.395290Z",
     "iopub.status.busy": "2024-10-07T12:38:30.394706Z",
     "iopub.status.idle": "2024-10-07T12:38:31.033699Z",
     "shell.execute_reply": "2024-10-07T12:38:31.032156Z",
     "shell.execute_reply.started": "2024-10-07T12:38:30.395223Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using `pandas` for handling and manipulating structured data in the form of DataFrames, and duckdb to run SQL queries efficiently on these DataFrames, combining the power of SQL with the flexibility of Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Established a connection to DuckDB for running SQL queries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T12:38:32.999983Z",
     "iopub.status.busy": "2024-10-07T12:38:32.999284Z",
     "iopub.status.idle": "2024-10-07T12:38:33.016286Z",
     "shell.execute_reply": "2024-10-07T12:38:33.014530Z",
     "shell.execute_reply.started": "2024-10-07T12:38:32.999912Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "duckdb_conn = duckdb.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a connection to DuckDB to execute SQL queries within the notebook. This connection will allow me to query the datasets loaded into memory using SQL syntax, leveraging DuckDBâ€™s performance for efficient data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loaded the music store dataset from CSV files into pandas DataFrames.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T12:38:35.761747Z",
     "iopub.status.busy": "2024-10-07T12:38:35.761070Z",
     "iopub.status.idle": "2024-10-07T12:38:35.931840Z",
     "shell.execute_reply": "2024-10-07T12:38:35.930548Z",
     "shell.execute_reply.started": "2024-10-07T12:38:35.761663Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "album_df = pd.read_csv(r'C:\\Users\\saswa\\Documents\\GitHub\\CUO-On-Site-Internship--Music-Store-Database-Evaluation-and-Optimization\\Data\\music_store_database\\album.csv')\n",
    "artist_df = pd.read_csv(r'C:\\Users\\saswa\\Documents\\GitHub\\CUO-On-Site-Internship--Music-Store-Database-Evaluation-and-Optimization\\Data\\music_store_database\\artist.csv')\n",
    "customer_df = pd.read_csv(r'C:\\Users\\saswa\\Documents\\GitHub\\CUO-On-Site-Internship--Music-Store-Database-Evaluation-and-Optimization\\Data\\music_store_database\\customer.csv')\n",
    "employee_df = pd.read_csv(r'C:\\Users\\saswa\\Documents\\GitHub\\CUO-On-Site-Internship--Music-Store-Database-Evaluation-and-Optimization\\Data\\music_store_database\\employee.csv')\n",
    "genre_df = pd.read_csv(r'C:\\Users\\saswa\\Documents\\GitHub\\CUO-On-Site-Internship--Music-Store-Database-Evaluation-and-Optimization\\Data\\music_store_database\\genre.csv')\n",
    "invoice_df = pd.read_csv(r'C:\\Users\\saswa\\Documents\\GitHub\\CUO-On-Site-Internship--Music-Store-Database-Evaluation-and-Optimization\\Data\\music_store_database\\invoice.csv')\n",
    "invoice_line_df = pd.read_csv(r'C:\\Users\\saswa\\Documents\\GitHub\\CUO-On-Site-Internship--Music-Store-Database-Evaluation-and-Optimization\\Data\\music_store_database\\invoice_line.csv')\n",
    "media_type_df = pd.read_csv(r'C:\\Users\\saswa\\Documents\\GitHub\\CUO-On-Site-Internship--Music-Store-Database-Evaluation-and-Optimization\\Data\\music_store_database\\media_type.csv')\n",
    "playlist_df = pd.read_csv(r'C:\\Users\\saswa\\Documents\\GitHub\\CUO-On-Site-Internship--Music-Store-Database-Evaluation-and-Optimization\\Data\\music_store_database\\playlist.csv')\n",
    "playlist_track_df = pd.read_csv(r'C:\\Users\\saswa\\Documents\\GitHub\\CUO-On-Site-Internship--Music-Store-Database-Evaluation-and-Optimization\\Data\\music_store_database\\playlist_track.csv')\n",
    "track_df = pd.read_csv(r'C:\\Users\\saswa\\Documents\\GitHub\\CUO-On-Site-Internship--Music-Store-Database-Evaluation-and-Optimization\\Data\\music_store_database\\track.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I loaded each table from the music store database as separate CSV files into pandas DataFrames. This allows me to easily manipulate, analyze, and query the data using SQL queries with duckdb. Each DataFrame corresponds to a specific entity in the database, such as albums, artists, customers, invoices, and tracks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouped the DataFrames into a dictionary for streamlined future operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T12:38:39.365857Z",
     "iopub.status.busy": "2024-10-07T12:38:39.365332Z",
     "iopub.status.idle": "2024-10-07T12:38:39.374555Z",
     "shell.execute_reply": "2024-10-07T12:38:39.372731Z",
     "shell.execute_reply.started": "2024-10-07T12:38:39.365810Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataframes = {\n",
    "    \"albums\": album_df,\n",
    "    \"artists\": artist_df,\n",
    "    \"customers\": customer_df,\n",
    "    \"employees\": employee_df,\n",
    "    \"genres\": genre_df,\n",
    "    \"invoices\": invoice_df,\n",
    "    \"invoice_lines\": invoice_line_df,\n",
    "    \"media_types\": media_type_df,\n",
    "    \"playlists\": playlist_df,\n",
    "    \"playlist_tracks\": playlist_track_df,\n",
    "    \"tracks\": track_df,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an optional step, I organized all the DataFrames into a Python dictionary. This structure makes it easier to iterate over the DataFrames if I need to apply repetitive tasks or transformations in the future. Each key in the dictionary corresponds to a table from the music store database, and the values are the associated DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an optional step, I organized all the DataFrames into a Python dictionary. This structure makes it easier to iterate over the DataFrames if I need to apply repetitive tasks or transformations in the future. Each key in the dictionary corresponds to a table from the music store database, and the values are the associated DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an optional step, I organized all the DataFrames into a Python dictionary. This structure makes it easier to iterate over the DataFrames if I need to apply repetitive tasks or transformations in the future. Each key in the dictionary corresponds to a table from the music store database, and the values are the associated DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Registered the DataFrames with DuckDB for SQL querying.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T12:38:42.165477Z",
     "iopub.status.busy": "2024-10-07T12:38:42.164535Z",
     "iopub.status.idle": "2024-10-07T12:38:42.235983Z",
     "shell.execute_reply": "2024-10-07T12:38:42.234281Z",
     "shell.execute_reply.started": "2024-10-07T12:38:42.165422Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x1d35e877570>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb_conn.register('album', album_df)\n",
    "duckdb_conn.register('artist', artist_df)\n",
    "duckdb_conn.register('customer', customer_df)\n",
    "duckdb_conn.register('employee', employee_df)\n",
    "duckdb_conn.register('genre', genre_df)\n",
    "duckdb_conn.register('invoice', invoice_df)\n",
    "duckdb_conn.register('invoice_line', invoice_line_df)\n",
    "duckdb_conn.register('media_type', media_type_df)\n",
    "duckdb_conn.register('playlist', playlist_df)\n",
    "duckdb_conn.register('playlist_track', playlist_track_df)\n",
    "duckdb_conn.register('track', track_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I registered each pandas DataFrame with DuckDB, making them accessible for SQL queries within the DuckDB environment. This allows me to use SQL syntax to query and analyze the data stored in these DataFrames efficiently, as if they were SQL tables. Each DataFrame is given an alias (e.g., `album`, `artist`), which will be used in subsequent SQL queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question Set 1**\n",
    "\n",
    "1. Who is the senior most employee based on job title?\n",
    "\n",
    "**Solution 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T12:38:56.134420Z",
     "iopub.status.busy": "2024-10-07T12:38:56.134027Z",
     "iopub.status.idle": "2024-10-07T12:38:56.153745Z",
     "shell.execute_reply": "2024-10-07T12:38:56.152025Z",
     "shell.execute_reply.started": "2024-10-07T12:38:56.134382Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   employee_id first_name last_name\n",
      "0            9      Mohan     Madan\n"
     ]
    }
   ],
   "source": [
    "result = duckdb_conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        employee_id, \n",
    "        first_name, \n",
    "        last_name \n",
    "    FROM \n",
    "        employee \n",
    "    ORDER BY \n",
    "        levels DESC \n",
    "    LIMIT 1\n",
    "\"\"\").df()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intuition for Solution 1:**\n",
    "\n",
    "* Identified the Senior-Most Employee: To determine the senior-most employee, I used the `levels` column, which reflects the job level of each employee. Higher levels indicate more senior positions.\n",
    "\n",
    "* Sorting: By sorting the `levels` column in descending order (`DESC`), the employee with the highest job level appeared first in the result set.\n",
    "\n",
    "* Limiting the Results: The `LIMIT 1` ensured that only one record (the topmost, senior-most employee) was returned.\n",
    "\n",
    "---\n",
    "\n",
    "**Explanation for Solution 1:**\n",
    "\n",
    "* ORDER BY levels DESC: This sorted the employees based on their job level in descending order, meaning the employee with the highest level (i.e., most senior) appeared first.\n",
    "\n",
    "* LIMIT 1: This restricted the result to the top-most record after sorting, so only the senior-most employee was returned.\n",
    "\n",
    "* The query selected and returned the `employee_id`, `first_name`, and `last_name` of the most senior employee.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T12:42:12.636416Z",
     "iopub.status.busy": "2024-10-07T12:42:12.635908Z",
     "iopub.status.idle": "2024-10-07T12:42:12.650381Z",
     "shell.execute_reply": "2024-10-07T12:42:12.648837Z",
     "shell.execute_reply.started": "2024-10-07T12:42:12.636372Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   employee_id first_name\n",
      "0            9      Mohan\n"
     ]
    }
   ],
   "source": [
    "result = duckdb_conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        employee_id, \n",
    "        first_name \n",
    "    FROM \n",
    "        employee \n",
    "    WHERE \n",
    "        employee_id = '9'\n",
    "\"\"\").df()\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intuition for Solution 2:**\n",
    "\n",
    "* Direct Employee Lookup: This query looked up the employee information directly by using their `employee_id`. The value `'9'` was assumed to be the specific employee ID in question.\n",
    "\n",
    "* Efficient Query: The query used a direct search by `employee_id`, which was usually the primary key, ensuring that the result was retrieved quickly.\n",
    "\n",
    "---\n",
    "\n",
    "**Explanation for Solution 2:**\n",
    "\n",
    "* WHERE employee_id = '9': This clause filtered the result to the employee whose `employee_id` was `'9'`. Since `employee_id` is typically a primary key, this operation was very efficient.\n",
    "\n",
    "* The query returned the `employee_id` and `first_name` of the employee with the specified `employee_id`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Which countries have the most Invoices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T12:39:59.063714Z",
     "iopub.status.busy": "2024-10-07T12:39:59.063259Z",
     "iopub.status.idle": "2024-10-07T12:39:59.083416Z",
     "shell.execute_reply": "2024-10-07T12:39:59.081980Z",
     "shell.execute_reply.started": "2024-10-07T12:39:59.063673Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Country  Invoice_count\n",
      "0     USA            131\n",
      "1  Canada             76\n",
      "2  Brazil             61\n",
      "3  France             50\n"
     ]
    }
   ],
   "source": [
    "result = duckdb_conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        billing_country AS Country, \n",
    "        COUNT(invoice_id) AS Invoice_count \n",
    "    FROM \n",
    "        invoice \n",
    "    GROUP BY \n",
    "        Country \n",
    "    ORDER BY \n",
    "        Invoice_count DESC \n",
    "    LIMIT 4\n",
    "\"\"\").df()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intuition:**\n",
    "\n",
    "* Grouped by Country: Since the query required me to find out which countries had the most invoices, I grouped the invoices by the `billing_country`.\n",
    "\n",
    "* Counted Invoices: By counting the `invoice_id` for each country, I determined how many invoices were generated per country.\n",
    "\n",
    "* Sorted by Invoice Count: To get the countries with the most invoices, I ordered the result set by `Invoice_count` in descending order.\n",
    "\n",
    "* Limited the Result: The query returned only the top 4 countries with the highest number of invoices using `LIMIT 4`.\n",
    "\n",
    "---\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* COUNT(invoice_id): This counted the number of invoices for each country. I used the `invoice_id` column to tally the number of invoices.\n",
    "\n",
    "* GROUP BY billing_country: This grouped the results by country, so the count was aggregated for each distinct `billing_country`.\n",
    "\n",
    "* ORDER BY Invoice_count DESC: The result set was sorted in descending order, meaning countries with the most invoices appeared first.\n",
    "\n",
    "* LIMIT 4: This restricted the output to the top 4 countries with the highest number of invoices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What are top 3 values of total invoice?\n",
    "\n",
    "**Solution 1-For top 3 values:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T12:41:24.042365Z",
     "iopub.status.busy": "2024-10-07T12:41:24.040482Z",
     "iopub.status.idle": "2024-10-07T12:41:24.060734Z",
     "shell.execute_reply": "2024-10-07T12:41:24.058995Z",
     "shell.execute_reply.started": "2024-10-07T12:41:24.042287Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   total\n",
      "0  23.76\n",
      "1  19.80\n",
      "2  19.80\n"
     ]
    }
   ],
   "source": [
    "result = duckdb_conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        total \n",
    "    FROM \n",
    "        invoice \n",
    "    ORDER BY \n",
    "        total DESC \n",
    "    LIMIT 3\n",
    "\"\"\").df()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intuition:**\n",
    "\n",
    "* Retrieved Total Invoice Amounts: The goal was to return the total invoice amounts in descending order.\n",
    "\n",
    "* Sorted by Total: Sorting by the `total` column in descending order ensured the highest totals appeared at the top.\n",
    "\n",
    "* Limited to 3 Rows: I limited the result to only 3 rows to get the top 3 total amounts, even if some values were repeated.\n",
    "\n",
    "---\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* ORDER BY total DESC: Sorting by the `total` in descending order ensured that the highest invoice values were prioritized.\n",
    "\n",
    "* LIMIT 3: This limited the result set to the top 3 rows, giving me the highest 3 total values, regardless of whether they were distinct.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution 2-For distinct top 3 values:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T12:43:08.915957Z",
     "iopub.status.busy": "2024-10-07T12:43:08.915386Z",
     "iopub.status.idle": "2024-10-07T12:43:08.934811Z",
     "shell.execute_reply": "2024-10-07T12:43:08.933145Z",
     "shell.execute_reply.started": "2024-10-07T12:43:08.915897Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   total\n",
      "0  23.76\n",
      "1  19.80\n",
      "2  18.81\n"
     ]
    }
   ],
   "source": [
    "result = duckdb_conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        DISTINCT total \n",
    "    FROM \n",
    "        invoice \n",
    "    ORDER BY \n",
    "        total DESC \n",
    "    LIMIT 3\n",
    "\"\"\").df()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intuition:**\n",
    "\n",
    "* Removed Duplicates: To ensure that only unique total values were returned, I used the `DISTINCT` keyword.\n",
    "\n",
    "* Sorted by Total: Sorting the unique totals in descending order ensured the highest distinct totals were shown first.\n",
    "\n",
    "* Limited to 3 Rows: I limited the result to 3 rows, ensuring that only the top 3 distinct invoice totals were returned.\n",
    "\n",
    "---\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* DISTINCT total: This ensured that only unique total invoice values were included in the result, filtering out any duplicate totals.\n",
    "\n",
    "* ORDER BY total DESC: Sorting by the `total` in descending order ensured that the highest unique totals were prioritized.\n",
    "\n",
    "* LIMIT 3: This limited the result set to the top 3 distinct total values, giving me the highest 3 unique invoice totals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Which city has the best customers? We would like to throw a promotional Music Festival in the city we made the most money. Write a query that returns one city that has the highest sum of invoice totals. Return both the city name & sum of all invoice totals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T12:43:58.161417Z",
     "iopub.status.busy": "2024-10-07T12:43:58.160920Z",
     "iopub.status.idle": "2024-10-07T12:43:58.180272Z",
     "shell.execute_reply": "2024-10-07T12:43:58.178769Z",
     "shell.execute_reply.started": "2024-10-07T12:43:58.161376Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          City  sum(total)\n",
      "0  Yellowknife       75.24\n"
     ]
    }
   ],
   "source": [
    "result = duckdb_conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        Billing_City AS City, \n",
    "        SUM(total) \n",
    "    FROM \n",
    "        invoice \n",
    "    GROUP BY \n",
    "        City \n",
    "    ORDER BY \n",
    "        City DESC \n",
    "    LIMIT 1\n",
    "\"\"\").df()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intuition:**\n",
    "\n",
    "* Grouped by City: I needed to sum up the total invoice amounts for each city, so I grouped the data by `billing_city`.\n",
    "\n",
    "* Calculated Total Revenue: The sum of all invoice totals for each city was calculated using `SUM(total)`.\n",
    "\n",
    "* Sorted by Revenue: Sorting by the total revenue in descending order ensured the city with the highest total revenue appeared first.\n",
    "\n",
    "* Returned the Top Result: I limited the result to one row to return the city with the highest total invoice sum.\n",
    "\n",
    "---\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* SUM(total): This aggregated the invoice totals for each city, giving me the total revenue for each billing city.\n",
    "\n",
    "* GROUP BY billing_city: Grouping by `billing_city` ensured that I calculated the total invoice amount separately for each city.\n",
    "\n",
    "* ORDER BY Total_Revenue DESC: Sorting the total revenue in descending order ensured the city with the highest total invoice amount came first.\n",
    "\n",
    "* LIMIT 1: I used `LIMIT 1` to return only the city with the highest total invoice sum, as I was only interested in the top city.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Who is the best customer? The customer who has spent the most money will be declared the best customer. Write a query that returns the person who has spent the most money.\n",
    "\n",
    "**Solution 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T12:47:48.681457Z",
     "iopub.status.busy": "2024-10-07T12:47:48.680940Z",
     "iopub.status.idle": "2024-10-07T12:47:48.705432Z",
     "shell.execute_reply": "2024-10-07T12:47:48.703679Z",
     "shell.execute_reply.started": "2024-10-07T12:47:48.681414Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  first_name    last_name   total\n",
      "0  FrantiÅ¡ek  WichterlovÃ¡  144.54\n"
     ]
    }
   ],
   "source": [
    "result = duckdb_conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        c.first_name AS first_name, \n",
    "        c.last_name AS last_name, \n",
    "        SUM(i.total) AS total \n",
    "    FROM \n",
    "        invoice AS i \n",
    "    JOIN \n",
    "        customer AS c ON i.customer_id = c.customer_id \n",
    "    GROUP BY \n",
    "        c.customer_id, c.first_name, c.last_name \n",
    "    ORDER BY \n",
    "        total DESC \n",
    "    LIMIT 1\n",
    "\"\"\").df()\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intuition:**\n",
    "\n",
    "* Identified Customer Spending: I needed to sum the total money spent by each customer, which was stored in the invoice table.\n",
    "\n",
    "* Joined Invoice and Customer Tables: By joining the invoice table with the customer table, I could map the total invoice amounts to the respective customers.\n",
    "\n",
    "* Grouped by Customer: I aggregated the total invoice amounts per customer using `GROUP BY` on `customer_id`.\n",
    "\n",
    "* Sorted by Total in Descending Order: Sorting the results by total spending in descending order helped me find the customer who had spent the most.\n",
    "\n",
    "* Returned the Best Customer: By using `LIMIT 1`, I returned the customer with the highest total spending.\n",
    "\n",
    "---\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* SUM(i.total): Calculated the total spending for each customer by summing up all the invoice totals associated with their `customer_id`.\n",
    "\n",
    "* JOIN customer and invoice: I joined the invoice table with the customer table to associate invoice totals with customer details.\n",
    "\n",
    "* GROUP BY c.customer_id: Grouped the sum of invoice totals by each customer to get the total spending per customer.\n",
    "\n",
    "* ORDER BY total DESC: Sorted the customers by their total spending in descending order, so the highest spender appeared first.\n",
    "\n",
    "* LIMIT 1: Ensured that only the top customer was returned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution 2:**\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "c.first_name AS first_name, c.last_name AS last_name, ROUND(CAST(SUM(i.total) AS NUMERIC), 2) AS total\n",
    "FROM\n",
    "invoice AS i\n",
    "JOIN\n",
    "customer AS c ON i.customer_id = c.customer_id\n",
    "GROUP BY\n",
    "c.customer_id\n",
    "ORDER BY\n",
    "total DESC\n",
    "LIMIT 1\n",
    "```\n",
    "\n",
    "For another solution to this question, I used the `ROUND` function with `SUM` to round the total spending to two decimal places, ensuring a cleaner display of the monetary value. All other aspects of the query, including the JOIN, GROUP BY, and ORDER BY clauses, remained consistent with Solution 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question Set 2\n",
    "\n",
    "1. Write query to return the email, first name, last name, & Genre of all Rock Music listeners. Return your list ordered alphabetically by email starting with A.\n",
    "\n",
    "**Solution 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T12:55:50.362471Z",
     "iopub.status.busy": "2024-10-07T12:55:50.360968Z",
     "iopub.status.idle": "2024-10-07T12:55:50.399401Z",
     "shell.execute_reply": "2024-10-07T12:55:50.397896Z",
     "shell.execute_reply.started": "2024-10-07T12:55:50.362416Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Email First_name     Last_name genre\n",
      "0          aaronmitchell@yahoo.ca      Aaron      Mitchell  Rock\n",
      "1                alero@uol.com.br  Alexandre         Rocha  Rock\n",
      "2          astrid.gruber@apple.at     Astrid        Gruber  Rock\n",
      "3           bjorn.hansen@yahoo.no      BjÃ¸rn        Hansen  Rock\n",
      "4        camille.bernard@yahoo.fr    Camille       Bernard  Rock\n",
      "5           daan_peeters@apple.be       Daan       Peeters  Rock\n",
      "6        diego.gutierrez@yahoo.ar      Diego     GutiÃ©rrez  Rock\n",
      "7             dmiller@comcast.com        Dan        Miller  Rock\n",
      "8     dominiquelefebvre@gmail.com  Dominique      Lefebvre  Rock\n",
      "9             edfrancis@yachoo.ca     Edward       Francis  Rock\n",
      "10       eduardo@woodstock.com.br    Eduardo       Martins  Rock\n",
      "11         ellie.sullivan@shaw.ca      Ellie      Sullivan  Rock\n",
      "12         emma_jones@hotmail.com       Emma         Jones  Rock\n",
      "13         enrique_munoz@yahoo.es    Enrique         MuÃ±oz  Rock\n",
      "14       fernadaramos4@uol.com.br   Fernanda         Ramos  Rock\n",
      "15             fharris@google.com      Frank        Harris  Rock\n",
      "16             fralston@gmail.com      Frank       Ralston  Rock\n",
      "17       frantisekw@jetbrains.com  FrantiÅ¡ek   WichterlovÃ¡  Rock\n",
      "18            ftremblay@gmail.com   FranÃ§ois      Tremblay  Rock\n",
      "19           fzimmermann@yahoo.de       Fynn    Zimmermann  Rock\n",
      "20      hannah.schneider@yahoo.de     Hannah     Schneider  Rock\n",
      "21                hholy@gmail.com     Helena          HolÃ½  Rock\n",
      "22             hleacock@gmail.com    Heather       Leacock  Rock\n",
      "23           hughoreilly@apple.ie       Hugh      O'Reilly  Rock\n",
      "24      isabelle_mercier@apple.fr   Isabelle       Mercier  Rock\n",
      "25        jacksmith@microsoft.com       Jack         Smith  Rock\n",
      "26            jenniferp@rogers.ca   Jennifer      Peterson  Rock\n",
      "27            jfernandes@yahoo.pt       JoÃ£o     Fernandes  Rock\n",
      "28      joakim.johansson@yahoo.se     Joakim     Johansson  Rock\n",
      "29        johavanderberg@yahoo.nl   Johannes  Van der Berg  Rock\n",
      "30         johngordon22@yahoo.com       John        Gordon  Rock\n",
      "31            jubarnett@gmail.com      Julia       Barnett  Rock\n",
      "32            kachase@hotmail.com      Kathy         Chase  Rock\n",
      "33          kara.nielsen@jubii.dk       Kara       Nielsen  Rock\n",
      "34       ladislav_kovacs@apple.hu   Ladislav        KovÃ¡cs  Rock\n",
      "35          leonekohler@surfeu.de     Leonie        KÃ¶hler  Rock\n",
      "36         lucas.mancini@yahoo.it      Lucas       Mancini  Rock\n",
      "37           luisg@embraer.com.br       LuÃ­s     GonÃ§alves  Rock\n",
      "38             luisrojas@yahoo.cl       Luis         Rojas  Rock\n",
      "39        manoj.pareek@rediff.com      Manoj        Pareek  Rock\n",
      "40        marc.dubois@hotmail.com       Marc        Dubois  Rock\n",
      "41           mark.taylor@yahoo.au       Mark        Taylor  Rock\n",
      "42           marthasilk@gmail.com     Martha          Silk  Rock\n",
      "43              masampaio@sapo.pt   Madalena       Sampaio  Rock\n",
      "44              michelleb@aol.com   Michelle        Brooks  Rock\n",
      "45             mphilips12@shaw.ca       Mark       Philips  Rock\n",
      "46            nschroder@surfeu.de     Niklas      SchrÃ¶der  Rock\n",
      "47           patrick.gray@aol.com    Patrick          Gray  Rock\n",
      "48          phil.hughes@gmail.com       Phil        Hughes  Rock\n",
      "49       ricunningham@hotmail.com    Richard    Cunningham  Rock\n",
      "50        rishabh_mishra@yahoo.in    Rishabh        Mishra  Rock\n",
      "51               robbrown@shaw.ca     Robert         Brown  Rock\n",
      "52  roberto.almeida@riotur.gov.br    Roberto       Almeida  Rock\n",
      "53         stanisÅ‚aw.wÃ³jcik@wp.pl  StanisÅ‚aw        WÃ³jcik  Rock\n",
      "54          steve.murray@yahoo.uk      Steve        Murray  Rock\n",
      "55      terhi.hamalainen@apple.fi      Terhi    HÃ¤mÃ¤lÃ¤inen  Rock\n",
      "56               tgoyer@apple.com        Tim         Goyer  Rock\n",
      "57             vstevens@yahoo.com     Victor       Stevens  Rock\n",
      "58          wyatt.girard@yahoo.fr      Wyatt        Girard  Rock\n"
     ]
    }
   ],
   "source": [
    "result = duckdb_conn.execute(\"\"\"\n",
    "    WITH CTE AS (\n",
    "        SELECT c.email, c.first_name, c.last_name, g.name as genre\n",
    "        FROM customer AS c \n",
    "        JOIN invoice AS i ON c.customer_id = i.customer_id \n",
    "        JOIN invoice_line AS il ON i.invoice_id = il.invoice_id \n",
    "        JOIN track AS t ON t.track_id = il.track_id \n",
    "        JOIN genre AS g ON t.genre_id = g.genre_id\n",
    "    )\n",
    "\n",
    "    SELECT \n",
    "        DISTINCT email AS Email,\n",
    "        first_name AS First_name,\n",
    "        last_name AS Last_name,\n",
    "        genre\n",
    "    FROM \n",
    "        CTE \n",
    "    WHERE \n",
    "        genre = 'Rock'\n",
    "    ORDER BY \n",
    "        Email;\n",
    "\"\"\").df()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution 1:**\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "* Identified Customers Who Listen to Rock: I needed to join various tables such as customer, invoice, invoice_line, track, and genre to identify customers who had purchased tracks in the 'Rock' genre.\n",
    "\n",
    "* Filtered for the 'Rock' Genre: After joining the tables, I filtered the data by the 'Rock' genre.\n",
    "\n",
    "* Removed Duplicates: Using `DISTINCT` ensured that I did not get duplicate entries for customers who might have purchased multiple Rock tracks.\n",
    "\n",
    "* Ordered by Email: The result was sorted alphabetically by the email column.\n",
    "\n",
    "---\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* WITH CTE AS: I used a common table expression (CTE) to simplify the query. This CTE joined the customer, invoice, invoice_line, track, and genre tables to obtain customer details along with their genre preferences.\n",
    "\n",
    "* DISTINCT: I applied the `DISTINCT` clause in the final `SELECT` to eliminate any duplicate rows that might occur if a customer purchased multiple tracks in the same genre.\n",
    "\n",
    "* Filtered by 'Rock': The `WHERE genre = 'Rock'` condition ensured that I only returned customers who had listened to Rock music.\n",
    "\n",
    "* ORDER BY Email: Finally, I ordered the results alphabetically by the email field to match the requirement of sorting the list by email starting from 'A'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution 2-To optimise:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T12:56:32.841880Z",
     "iopub.status.busy": "2024-10-07T12:56:32.841346Z",
     "iopub.status.idle": "2024-10-07T12:56:32.875523Z",
     "shell.execute_reply": "2024-10-07T12:56:32.874291Z",
     "shell.execute_reply.started": "2024-10-07T12:56:32.841832Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Email First_name     Last_name genre\n",
      "0          aaronmitchell@yahoo.ca      Aaron      Mitchell  Rock\n",
      "1                alero@uol.com.br  Alexandre         Rocha  Rock\n",
      "2          astrid.gruber@apple.at     Astrid        Gruber  Rock\n",
      "3           bjorn.hansen@yahoo.no      BjÃ¸rn        Hansen  Rock\n",
      "4        camille.bernard@yahoo.fr    Camille       Bernard  Rock\n",
      "5           daan_peeters@apple.be       Daan       Peeters  Rock\n",
      "6        diego.gutierrez@yahoo.ar      Diego     GutiÃ©rrez  Rock\n",
      "7             dmiller@comcast.com        Dan        Miller  Rock\n",
      "8     dominiquelefebvre@gmail.com  Dominique      Lefebvre  Rock\n",
      "9             edfrancis@yachoo.ca     Edward       Francis  Rock\n",
      "10       eduardo@woodstock.com.br    Eduardo       Martins  Rock\n",
      "11         ellie.sullivan@shaw.ca      Ellie      Sullivan  Rock\n",
      "12         emma_jones@hotmail.com       Emma         Jones  Rock\n",
      "13         enrique_munoz@yahoo.es    Enrique         MuÃ±oz  Rock\n",
      "14       fernadaramos4@uol.com.br   Fernanda         Ramos  Rock\n",
      "15             fharris@google.com      Frank        Harris  Rock\n",
      "16             fralston@gmail.com      Frank       Ralston  Rock\n",
      "17       frantisekw@jetbrains.com  FrantiÅ¡ek   WichterlovÃ¡  Rock\n",
      "18            ftremblay@gmail.com   FranÃ§ois      Tremblay  Rock\n",
      "19           fzimmermann@yahoo.de       Fynn    Zimmermann  Rock\n",
      "20      hannah.schneider@yahoo.de     Hannah     Schneider  Rock\n",
      "21                hholy@gmail.com     Helena          HolÃ½  Rock\n",
      "22             hleacock@gmail.com    Heather       Leacock  Rock\n",
      "23           hughoreilly@apple.ie       Hugh      O'Reilly  Rock\n",
      "24      isabelle_mercier@apple.fr   Isabelle       Mercier  Rock\n",
      "25        jacksmith@microsoft.com       Jack         Smith  Rock\n",
      "26            jenniferp@rogers.ca   Jennifer      Peterson  Rock\n",
      "27            jfernandes@yahoo.pt       JoÃ£o     Fernandes  Rock\n",
      "28      joakim.johansson@yahoo.se     Joakim     Johansson  Rock\n",
      "29        johavanderberg@yahoo.nl   Johannes  Van der Berg  Rock\n",
      "30         johngordon22@yahoo.com       John        Gordon  Rock\n",
      "31            jubarnett@gmail.com      Julia       Barnett  Rock\n",
      "32            kachase@hotmail.com      Kathy         Chase  Rock\n",
      "33          kara.nielsen@jubii.dk       Kara       Nielsen  Rock\n",
      "34       ladislav_kovacs@apple.hu   Ladislav        KovÃ¡cs  Rock\n",
      "35          leonekohler@surfeu.de     Leonie        KÃ¶hler  Rock\n",
      "36         lucas.mancini@yahoo.it      Lucas       Mancini  Rock\n",
      "37           luisg@embraer.com.br       LuÃ­s     GonÃ§alves  Rock\n",
      "38             luisrojas@yahoo.cl       Luis         Rojas  Rock\n",
      "39        manoj.pareek@rediff.com      Manoj        Pareek  Rock\n",
      "40        marc.dubois@hotmail.com       Marc        Dubois  Rock\n",
      "41           mark.taylor@yahoo.au       Mark        Taylor  Rock\n",
      "42           marthasilk@gmail.com     Martha          Silk  Rock\n",
      "43              masampaio@sapo.pt   Madalena       Sampaio  Rock\n",
      "44              michelleb@aol.com   Michelle        Brooks  Rock\n",
      "45             mphilips12@shaw.ca       Mark       Philips  Rock\n",
      "46            nschroder@surfeu.de     Niklas      SchrÃ¶der  Rock\n",
      "47           patrick.gray@aol.com    Patrick          Gray  Rock\n",
      "48          phil.hughes@gmail.com       Phil        Hughes  Rock\n",
      "49       ricunningham@hotmail.com    Richard    Cunningham  Rock\n",
      "50        rishabh_mishra@yahoo.in    Rishabh        Mishra  Rock\n",
      "51               robbrown@shaw.ca     Robert         Brown  Rock\n",
      "52  roberto.almeida@riotur.gov.br    Roberto       Almeida  Rock\n",
      "53         stanisÅ‚aw.wÃ³jcik@wp.pl  StanisÅ‚aw        WÃ³jcik  Rock\n",
      "54          steve.murray@yahoo.uk      Steve        Murray  Rock\n",
      "55      terhi.hamalainen@apple.fi      Terhi    HÃ¤mÃ¤lÃ¤inen  Rock\n",
      "56               tgoyer@apple.com        Tim         Goyer  Rock\n",
      "57             vstevens@yahoo.com     Victor       Stevens  Rock\n",
      "58          wyatt.girard@yahoo.fr      Wyatt        Girard  Rock\n"
     ]
    }
   ],
   "source": [
    "result = duckdb_conn.execute(\"\"\"\n",
    "    WITH CTE AS (\n",
    "        SELECT DISTINCT c.email, c.first_name, c.last_name, g.name AS genre\n",
    "        FROM customer AS c \n",
    "        JOIN invoice AS i ON c.customer_id = i.customer_id \n",
    "        JOIN invoice_line AS il ON i.invoice_id = il.invoice_id \n",
    "        JOIN track AS t ON t.track_id = il.track_id \n",
    "        JOIN genre AS g ON t.genre_id = g.genre_id\n",
    "    )\n",
    "    SELECT \n",
    "        email AS Email,\n",
    "        first_name AS First_name,\n",
    "        last_name AS Last_name,\n",
    "        genre\n",
    "    FROM \n",
    "        CTE \n",
    "    WHERE \n",
    "        genre = 'Rock'\n",
    "    ORDER BY \n",
    "        Email;\n",
    "\"\"\").df()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution 2:**\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "* Pre-filtered Duplicates in CTE: In this optimized version, I applied the `DISTINCT` keyword inside the CTE itself, reducing the result set early on and minimizing unnecessary processing in the final query.\n",
    "\n",
    "* Same Logic for Filtering and Ordering: The logic for filtering by the 'Rock' genre and ordering by email remained the same.\n",
    "\n",
    "---\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* DISTINCT in the CTE: This query applied `DISTINCT` in the CTE, meaning duplicates were removed early on, which improved performance in large datasets.\n",
    "\n",
    "* Final Query: The final `SELECT` and filtering logic remained the same, with the goal of returning the required customer details and genre, ordered by email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Let's invite the artists who have written the most rock music in our dataset. Write a query that returns the Artist name and total track count of the top 10 rock bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T12:58:29.578540Z",
     "iopub.status.busy": "2024-10-07T12:58:29.578086Z",
     "iopub.status.idle": "2024-10-07T12:58:29.600018Z",
     "shell.execute_reply": "2024-10-07T12:58:29.598759Z",
     "shell.execute_reply.started": "2024-10-07T12:58:29.578484Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    artist_name  rock_track_cnt\n",
      "0                  Led Zeppelin             114\n",
      "1                            U2             112\n",
      "2                   Deep Purple              92\n",
      "3                   Iron Maiden              81\n",
      "4                     Pearl Jam              54\n",
      "5                     Van Halen              52\n",
      "6                         Queen              45\n",
      "7            The Rolling Stones              41\n",
      "8  Creedence Clearwater Revival              40\n",
      "9                          Kiss              35\n"
     ]
    }
   ],
   "source": [
    "result = duckdb_conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        ar.name AS artist_name, \n",
    "        COUNT(t.track_id) AS rock_track_cnt \n",
    "    FROM \n",
    "        track AS t \n",
    "    JOIN \n",
    "        album AS a ON t.album_id = a.album_id \n",
    "    JOIN \n",
    "        artist AS ar ON a.artist_id = ar.artist_id \n",
    "    GROUP BY \n",
    "        ar.name,t.genre_id \n",
    "    HAVING \n",
    "        t.genre_id = '1' \n",
    "    ORDER BY \n",
    "        rock_track_cnt DESC \n",
    "    LIMIT 10;\n",
    "\"\"\").df()\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intuition:**\n",
    "\n",
    "* Identified Rock Tracks: I filtered for tracks that belonged to the 'Rock' genre. I used the `genre_id = 1` assumption based on the provided schema, where each genre had a unique ID.\n",
    "\n",
    "* Grouped by Artist: I counted the total number of Rock tracks for each artist by grouping the results by the artist's name and filtering specifically for the 'Rock' genre.\n",
    "\n",
    "* Ordered by Track Count: To identify the top 10 artists who composed the most Rock music, I ordered the results by the total number of Rock tracks in descending order.\n",
    "\n",
    "---\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* Table Joins: I joined the track table with the album table using the `album_id`, and then the album table with the artist table using `artist_id`. This allowed me to link each track with the corresponding artist.\n",
    "\n",
    "* Grouping and Filtering: After joining, I grouped the tracks by artist name and `genre_id`, which allowed me to aggregate the track count for each artist specifically within the 'Rock' genre.\n",
    "\n",
    "* Filtering by Genre: The `HAVING t.genre_id = '1'` condition ensured I counted only tracks that belonged to the Rock genre.\n",
    "\n",
    "* Ordering and Limiting: I sorted the artists by their Rock track count in descending order and limited the result to the top 10 artists.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Return all the track names that have a song length longer than the average song length. Return the Name and Milliseconds for each track. Order by the song length with the longest songs listed first.\n",
    "\n",
    "**Solution 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T12:59:41.323871Z",
     "iopub.status.busy": "2024-10-07T12:59:41.323290Z",
     "iopub.status.idle": "2024-10-07T12:59:41.356971Z",
     "shell.execute_reply": "2024-10-07T12:59:41.355443Z",
     "shell.execute_reply.started": "2024-10-07T12:59:41.323823Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  name  Minutes\n",
      "0                               Occupation / Precipice  5286953\n",
      "1                              Through a Looking Glass  5088838\n",
      "2                          Greetings from Earth, Pt. 1  2960293\n",
      "3                              The Man With Nine Lives  2956998\n",
      "4                          Battlestar Galactica, Pt. 2  2956081\n",
      "..                                                 ...      ...\n",
      "489                                   22 Acacia Avenue   395572\n",
      "490                                  The Unforgiven II   395520\n",
      "491                                 The Shortest Straw   395389\n",
      "492  Concerto for Clarinet in A Major, K. 622: II. ...   394482\n",
      "493                                        Wicked Ways   393691\n",
      "\n",
      "[494 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "result = duckdb_conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        \"name\", \n",
    "        Milliseconds AS Minutes \n",
    "    FROM \n",
    "        track \n",
    "    WHERE \n",
    "        Milliseconds > (SELECT AVG(Milliseconds) FROM track) \n",
    "    ORDER BY \n",
    "        Milliseconds DESC;\n",
    "\"\"\").df()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intuition:**\n",
    "\n",
    "* Average Song Length: I calculated the average song length in milliseconds using a subquery `(SELECT AVG(Milliseconds) FROM Track)`.\n",
    "\n",
    "* Filter for Longer Songs: I filtered tracks whose length in milliseconds exceeded the average.\n",
    "\n",
    "* Order by Length: I ordered the results in descending order based on the length in milliseconds to show the longest tracks first.\n",
    "\n",
    "---\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* Subquery for Average Length: The subquery `(SELECT AVG(Milliseconds) FROM Track)` calculated the average duration of all tracks in the Track table. This result was used in the WHERE clause to filter for songs that were longer than the average.\n",
    "\n",
    "* Filtering: The main query selected the tracks whose Milliseconds value exceeded the calculated average.\n",
    "\n",
    "* Ordering: The results were ordered by `Milliseconds DESC` to list the longest tracks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T13:00:21.873120Z",
     "iopub.status.busy": "2024-10-07T13:00:21.872567Z",
     "iopub.status.idle": "2024-10-07T13:00:21.903997Z",
     "shell.execute_reply": "2024-10-07T13:00:21.902393Z",
     "shell.execute_reply.started": "2024-10-07T13:00:21.873072Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  name    Minutes\n",
      "0                               Occupation / Precipice  88.115883\n",
      "1                              Through a Looking Glass  84.813967\n",
      "2                          Greetings from Earth, Pt. 1  49.338217\n",
      "3                              The Man With Nine Lives  49.283300\n",
      "4                          Battlestar Galactica, Pt. 2  49.268017\n",
      "..                                                 ...        ...\n",
      "489                                   22 Acacia Avenue   6.592867\n",
      "490                                  The Unforgiven II   6.592000\n",
      "491                                 The Shortest Straw   6.589817\n",
      "492  Concerto for Clarinet in A Major, K. 622: II. ...   6.574700\n",
      "493                                        Wicked Ways   6.561517\n",
      "\n",
      "[494 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "result = duckdb_conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        \"name\", \n",
    "        Milliseconds / 60000 AS Minutes \n",
    "    FROM \n",
    "        track \n",
    "    WHERE \n",
    "        Milliseconds > (SELECT AVG(Milliseconds) FROM track) \n",
    "    ORDER BY \n",
    "        Milliseconds DESC;\n",
    "\"\"\").df()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intuition:**\n",
    "\n",
    "* Convert Milliseconds to Minutes: In this version of the query, I converted the Milliseconds field to minutes by dividing it by 60,000 (since there are 60,000 milliseconds in a minute).\n",
    "\n",
    "* Filter and Order: The filtering and ordering logic remained the same, based on the comparison to the average song length.\n",
    "\n",
    "---\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* Division for Minutes: The key difference in this query was the calculation `Milliseconds / 60000 AS Minutes`, which converted the track length from milliseconds to minutes for easier interpretation.\n",
    "\n",
    "* Rest of the Logic: Like the first query, I filtered tracks that were longer than the average duration and ordered them by length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question Set 3\n",
    "\n",
    "1. Find how much amount spent by each customer on artists? Write a query to return customer name, artist name and total spent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T13:01:34.293583Z",
     "iopub.status.busy": "2024-10-07T13:01:34.293048Z",
     "iopub.status.idle": "2024-10-07T13:01:34.331372Z",
     "shell.execute_reply": "2024-10-07T13:01:34.329820Z",
     "shell.execute_reply.started": "2024-10-07T13:01:34.293537Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     first_name   last_name         artist_name  total_spent\n",
      "0        Edward     Francis  The Rolling Stones        25.74\n",
      "1        Edward     Francis        Led Zeppelin        22.77\n",
      "2        Edward     Francis          The Police         9.90\n",
      "3        Martha        Silk       Frank Sinatra        18.81\n",
      "4     StanisÅ‚aw      WÃ³jcik       Gustav Mahler        10.89\n",
      "...         ...         ...                 ...          ...\n",
      "2184     Leonie      KÃ¶hler                UB40         7.92\n",
      "2185    Roberto     Almeida        Apocalyptica        10.89\n",
      "2186       John      Gordon         Soundgarden         7.92\n",
      "2187   Fernanda       Ramos      Zeca Pagodinho         6.93\n",
      "2188    Richard  Cunningham       Frank Sinatra         4.95\n",
      "\n",
      "[2189 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "result = duckdb_conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        c.first_name,\n",
    "        c.last_name, \n",
    "        ar.name AS artist_name,\n",
    "        ROUND(SUM(i.total), 2) AS total_spent\n",
    "    FROM \n",
    "        customer AS c\n",
    "    JOIN \n",
    "        invoice AS i ON c.customer_id = i.customer_id\n",
    "    JOIN \n",
    "        invoice_line AS il ON i.invoice_id = il.invoice_id\n",
    "    JOIN \n",
    "        track AS t ON il.track_id = t.track_id\n",
    "    JOIN \n",
    "        album AS a ON t.album_id = a.album_id\n",
    "    JOIN \n",
    "        artist AS ar ON a.artist_id = ar.artist_id\n",
    "    GROUP BY \n",
    "        c.first_name, \n",
    "        c.last_name, \n",
    "        ar.name;\n",
    "\"\"\").df()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intuition:**\n",
    "\n",
    "* Customer and Artist Relationship: I recognized that the relationship between customers and artists was indirect, going through multiple tables. The customer purchases invoices, which are linked to invoice_lines, and the tracks in these invoice lines are linked to specific albums that belong to artists.\n",
    "\n",
    "* Aggregate Spending: For each customer and artist combination, I needed to sum the total amount the customer spent across all purchases related to that artistâ€™s tracks.\n",
    "\n",
    "* Rounding the Total: The total spent was rounded to two decimal places for clarity.\n",
    "\n",
    "---\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* Table Joins: I started with the customer table (c) and joined it to the invoice table (i) using the customer_id. The invoice table was then joined to invoice_line (il), linking invoices to specific tracks. From the track table (t), I obtained the album_id, which linked to the album table (a), and finally to the artist table (ar) using the artist_id.\n",
    "\n",
    "* Group By: The query grouped by the customer's first and last name (c.first_name, c.last_name) and the artistâ€™s name (ar.name). This ensured that I calculated the total amount spent by each customer on each artist.\n",
    "\n",
    "* Total Calculation: I used `SUM(i.total)` to compute the total amount each customer had spent on the tracks by each artist. The result was rounded to two decimal places using `ROUND(SUM(i.total)::numeric, 2)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We want to find out the most popular music Genre for each country. We determine the most popular genre as the genre with the highest amount of purchases. Write a query that returns each country along with the top Genre. For countries where the maximum number of purchases is shared return all Genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T13:05:59.358874Z",
     "iopub.status.busy": "2024-10-07T13:05:59.358328Z",
     "iopub.status.idle": "2024-10-07T13:05:59.400986Z",
     "shell.execute_reply": "2024-10-07T13:05:59.399506Z",
     "shell.execute_reply.started": "2024-10-07T13:05:59.358831Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           country               genre  quantity\n",
      "0        Argentina  Alternative & Punk        17\n",
      "1        Australia                Rock        34\n",
      "2          Austria                Rock        40\n",
      "3          Belgium                Rock        26\n",
      "4           Brazil                Rock       205\n",
      "5           Canada                Rock       333\n",
      "6            Chile                Rock        61\n",
      "7   Czech Republic                Rock       143\n",
      "8          Denmark                Rock        24\n",
      "9          Finland                Rock        46\n",
      "10          France                Rock       211\n",
      "11         Germany                Rock       194\n",
      "12         Hungary                Rock        44\n",
      "13           India                Rock       102\n",
      "14         Ireland                Rock        72\n",
      "15           Italy                Rock        35\n",
      "16     Netherlands                Rock        33\n",
      "17          Norway                Rock        40\n",
      "18          Poland                Rock        40\n",
      "19        Portugal                Rock       108\n",
      "20           Spain                Rock        46\n",
      "21          Sweden                Rock        60\n",
      "22             USA                Rock       561\n",
      "23  United Kingdom                Rock       166\n"
     ]
    }
   ],
   "source": [
    "result = duckdb_conn.execute(\"\"\"\n",
    "    WITH CTE AS (\n",
    "        SELECT \n",
    "            c.country AS country, \n",
    "            g.name AS genre,\n",
    "            COUNT(*) AS quantity \n",
    "        FROM \n",
    "            customer AS c \n",
    "        JOIN \n",
    "            invoice AS i ON c.customer_id = i.customer_id \n",
    "        JOIN \n",
    "            invoice_line AS il ON i.invoice_id = il.invoice_id \n",
    "        JOIN \n",
    "            track AS t ON il.track_id = t.track_id \n",
    "        JOIN \n",
    "            genre AS g ON t.genre_id = g.genre_id\n",
    "        GROUP BY \n",
    "            c.country, g.name\n",
    "    ),\n",
    "    MaxQuantityPerCountry AS (\n",
    "        SELECT \n",
    "            country, \n",
    "            MAX(quantity) AS max_quantity\n",
    "        FROM \n",
    "            CTE\n",
    "        GROUP BY \n",
    "            country\n",
    "    )\n",
    "    SELECT \n",
    "        CTE.country, \n",
    "        CTE.genre, \n",
    "        CTE.quantity\n",
    "    FROM \n",
    "        CTE\n",
    "    JOIN \n",
    "        MaxQuantityPerCountry AS M\n",
    "        ON CTE.country = M.country AND CTE.quantity = M.max_quantity\n",
    "    ORDER BY \n",
    "        CTE.country;\n",
    "\"\"\").df()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intuition:**\n",
    "\n",
    "* Create CTE for Purchases: I started with a CTE to gather the count of purchases for each genre in each country.\n",
    "\n",
    "* Calculate Maximum Quantities: The second CTE, `MaxQuantityPerCountry`, captured the maximum quantity of purchases for each country, isolating the most popular genre(s).\n",
    "\n",
    "* Efficient Joining: I then joined the original CTE with this maximum quantity CTE, filtering down to those genres that had the maximum purchase count for their respective countries. This allowed me to retrieve all genres that were tied for popularity in a single pass.\n",
    "\n",
    "* Ordered Results: Finally, I returned the results, ordered by country.\n",
    "\n",
    "---\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* CTE Creation: The first CTE counted purchases for each genre and country.\n",
    "\n",
    "* Max Quantity CTE: The second CTE (`MaxQuantityPerCountry`) simplified the process of finding the maximum purchase quantity per country, helping to reduce the complexity of the final query.\n",
    "\n",
    "* Join for Final Results: By joining the original CTE with the maximum quantity results, I could easily retrieve all genres tied for the maximum purchase count in each country. This was often more efficient than using a WHERE clause for filtering.\n",
    "\n",
    "* Ordering: The results were ordered by country.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write a query that determines the customer that has spent the most on music for each country. Write a query that returns the country along with the top customer and how much they spent. For countries where the top amount spent is shared, provide all customers who spent this amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T13:08:13.939723Z",
     "iopub.status.busy": "2024-10-07T13:08:13.939172Z",
     "iopub.status.idle": "2024-10-07T13:08:13.973601Z",
     "shell.execute_reply": "2024-10-07T13:08:13.972077Z",
     "shell.execute_reply.started": "2024-10-07T13:08:13.939674Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           country first_name     last_name   total\n",
      "0        Argentina      Diego     GutiÃ©rrez   39.60\n",
      "1        Australia       Mark        Taylor   81.18\n",
      "2          Austria     Astrid        Gruber   69.30\n",
      "3          Belgium       Daan       Peeters   60.39\n",
      "4           Brazil       LuÃ­s     GonÃ§alves  108.90\n",
      "5           Canada   FranÃ§ois      Tremblay   99.99\n",
      "6            Chile       Luis         Rojas   97.02\n",
      "7   Czech Republic  FrantiÅ¡ek   WichterlovÃ¡  144.54\n",
      "8          Denmark       Kara       Nielsen   37.62\n",
      "9          Finland      Terhi    HÃ¤mÃ¤lÃ¤inen   79.20\n",
      "10          France      Wyatt        Girard   99.99\n",
      "11         Germany       Fynn    Zimmermann   94.05\n",
      "12         Hungary   Ladislav        KovÃ¡cs   78.21\n",
      "13           India      Manoj        Pareek  111.87\n",
      "14         Ireland       Hugh      O'Reilly  114.84\n",
      "15           Italy      Lucas       Mancini   50.49\n",
      "16     Netherlands   Johannes  Van der Berg   65.34\n",
      "17          Norway      BjÃ¸rn        Hansen   72.27\n",
      "18          Poland  StanisÅ‚aw        WÃ³jcik   76.23\n",
      "19        Portugal       JoÃ£o     Fernandes  102.96\n",
      "20           Spain    Enrique         MuÃ±oz   98.01\n",
      "21          Sweden     Joakim     Johansson   75.24\n",
      "22             USA       Jack         Smith   98.01\n",
      "23  United Kingdom       Phil        Hughes   98.01\n"
     ]
    }
   ],
   "source": [
    "result = duckdb_conn.execute(\"\"\"\n",
    "    WITH CTE AS (\n",
    "        SELECT \n",
    "            c.country AS country, \n",
    "            c.first_name AS first_name, \n",
    "            c.last_name AS last_name, \n",
    "            ROUND(SUM(i.total)::numeric, 2) AS total\n",
    "        FROM \n",
    "            customer AS c \n",
    "        JOIN \n",
    "            invoice AS i ON c.customer_id = i.customer_id \n",
    "        GROUP BY \n",
    "            c.country, c.first_name, c.last_name\n",
    "    ),\n",
    "    MaxSpendingPerCountry AS (\n",
    "        SELECT \n",
    "            country, \n",
    "            MAX(total) AS max_total\n",
    "        FROM \n",
    "            CTE\n",
    "        GROUP BY \n",
    "            country\n",
    "    )\n",
    "    SELECT \n",
    "        CTE.country, \n",
    "        CTE.first_name, \n",
    "        CTE.last_name, \n",
    "        CTE.total\n",
    "    FROM \n",
    "        CTE\n",
    "    JOIN \n",
    "        MaxSpendingPerCountry AS M\n",
    "        ON CTE.country = M.country AND CTE.total = M.max_total\n",
    "    ORDER BY \n",
    "        CTE.country;\n",
    "\"\"\").df()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intuition:**\n",
    "\n",
    "* Aggregate and Max Calculation: The first CTE aggregated total spending per customer and country, while the second CTE calculated the maximum spending per country.\n",
    "\n",
    "* Join for Results: The final query joined the CTE with the maximum spending CTE to filter down to customers who had the maximum spending in their respective countries.\n",
    "\n",
    "* Efficiency: This method proved to be more efficient for larger datasets, as it clearly separated the aggregation and maximum calculations, leading to fewer rows in the final join.\n",
    "\n",
    "---\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* CTE Usage: The first CTE functioned similarly to the first solution, summing the total amounts spent by customers. The second CTE then retrieved the maximum spending for each country.\n",
    "\n",
    "* Joining for Top Customers: By joining the CTE with the maximum spending results, I efficiently identified all customers with the highest spending amounts without needing a complex filtering method.\n",
    "\n",
    "* Ordering: The results were ordered by country, ensuring that the output was structured for easy analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verifying Customers with Multiple Purchases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T13:08:45.169182Z",
     "iopub.status.busy": "2024-10-07T13:08:45.168691Z",
     "iopub.status.idle": "2024-10-07T13:08:45.203760Z",
     "shell.execute_reply": "2024-10-07T13:08:45.202260Z",
     "shell.execute_reply.started": "2024-10-07T13:08:45.169136Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           country first_name     last_name  cnt     tot\n",
      "0        Argentina      Diego     GutiÃ©rrez    5   39.60\n",
      "1        Australia       Mark        Taylor   10   81.18\n",
      "2          Austria     Astrid        Gruber    9   69.30\n",
      "3          Belgium       Daan       Peeters    7   60.39\n",
      "4           Brazil   Fernanda         Ramos   15  106.92\n",
      "5           Brazil       LuÃ­s     GonÃ§alves   13  108.90\n",
      "6           Brazil    Roberto       Almeida   11   82.17\n",
      "7           Brazil    Eduardo       Martins   12   60.39\n",
      "8           Brazil  Alexandre         Rocha   10   69.30\n",
      "9           Canada     Martha          Silk   11   62.37\n",
      "10          Canada       Mark       Philips   10   29.70\n",
      "11          Canada   FranÃ§ois      Tremblay    9   99.99\n",
      "12          Canada      Aaron      Mitchell    8   70.29\n",
      "13          Canada   Jennifer      Peterson    9   66.33\n",
      "14          Canada     Edward       Francis   13   91.08\n",
      "15          Canada      Ellie      Sullivan   12   75.24\n",
      "16          Canada     Robert         Brown    4   40.59\n",
      "17           Chile       Luis         Rojas   13   97.02\n",
      "18  Czech Republic     Helena          HolÃ½   12  128.70\n",
      "19  Czech Republic  FrantiÅ¡ek   WichterlovÃ¡   18  144.54\n",
      "20         Denmark       Kara       Nielsen   10   37.62\n",
      "21         Finland      Terhi    HÃ¤mÃ¤lÃ¤inen   11   79.20\n",
      "22          France       Marc        Dubois    9   64.35\n",
      "23          France  Dominique      Lefebvre    9   72.27\n",
      "24          France   Isabelle       Mercier   12   73.26\n",
      "25          France      Wyatt        Girard   11   99.99\n",
      "26          France    Camille       Bernard    9   79.20\n",
      "27         Germany       Fynn    Zimmermann   10   94.05\n",
      "28         Germany     Leonie        KÃ¶hler   11   82.17\n",
      "29         Germany     Niklas      SchrÃ¶der    9   73.26\n",
      "30         Germany     Hannah     Schneider   11   85.14\n",
      "31         Hungary   Ladislav        KovÃ¡cs   10   78.21\n",
      "32           India    Rishabh        Mishra    8   71.28\n",
      "33           India      Manoj        Pareek   13  111.87\n",
      "34         Ireland       Hugh      O'Reilly   13  114.84\n",
      "35           Italy      Lucas       Mancini    9   50.49\n",
      "36     Netherlands   Johannes  Van der Berg   10   65.34\n",
      "37          Norway      BjÃ¸rn        Hansen    9   72.27\n",
      "38          Poland  StanisÅ‚aw        WÃ³jcik   10   76.23\n",
      "39        Portugal       JoÃ£o     Fernandes   13  102.96\n",
      "40        Portugal   Madalena       Sampaio   16   82.17\n",
      "41           Spain    Enrique         MuÃ±oz   11   98.01\n",
      "42          Sweden     Joakim     Johansson   10   75.24\n",
      "43             USA    Patrick          Gray    9   84.15\n",
      "44             USA   Michelle        Brooks    8   79.20\n",
      "45             USA        Tim         Goyer    9   54.45\n",
      "46             USA       Jack         Smith   12   98.01\n",
      "47             USA      Frank       Ralston    8   71.28\n",
      "48             USA      Kathy         Chase   11   91.08\n",
      "49             USA      Julia       Barnett   10   72.27\n",
      "50             USA     Victor       Stevens   10   76.23\n",
      "51             USA        Dan        Miller   12   95.04\n",
      "52             USA    Richard    Cunningham   12   86.13\n",
      "53             USA    Heather       Leacock   12   92.07\n",
      "54             USA       John        Gordon   10   66.33\n",
      "55             USA      Frank        Harris    8   74.25\n",
      "56  United Kingdom       Phil        Hughes   11   98.01\n",
      "57  United Kingdom       Emma         Jones    8   68.31\n",
      "58  United Kingdom      Steve        Murray    9   79.20\n"
     ]
    }
   ],
   "source": [
    "result = duckdb_conn.execute(\"\"\"\n",
    "    WITH CTEE AS (\n",
    "        SELECT \n",
    "            c.country AS country, \n",
    "            c.first_name AS first_name, \n",
    "            c.last_name AS last_name, \n",
    "            COUNT(i.customer_id) AS cnt,\n",
    "            ROUND(SUM(i.total)::numeric, 2) AS tot\n",
    "        FROM \n",
    "            customer AS c \n",
    "        JOIN \n",
    "            invoice AS i ON c.customer_id = i.customer_id \n",
    "        GROUP BY \n",
    "            c.country, c.first_name, c.last_name\n",
    "    )\n",
    "    SELECT \n",
    "        country, \n",
    "        first_name, \n",
    "        last_name, \n",
    "        cnt,\n",
    "        tot\n",
    "    FROM \n",
    "        CTEE \n",
    "    WHERE \n",
    "        cnt > 1\n",
    "    ORDER BY \n",
    "        country;\n",
    "\"\"\").df()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "* Count Purchases: This query counted how many invoices each customer had and their total spending.\n",
    "\n",
    "* Filtering: It filtered to include only those customers who had more than one purchase, providing additional insights into customer behavior in each country.\n",
    "\n",
    "* Ordering: The results were sorted by country, helping to visualize customers with multiple purchases easily.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges Faced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout my internship, I encountered several challenges that tested my problem-solving skills and adaptability. A notable challenge was the meticulous data formatting required for a seamless import into the PostgreSQL database. While the PostgreSQL copy command offered a straightforward option, my preference for using import and export methods necessitated adjustments, particularly concerning the headers in the initial row of the CSV files. This situation demanded a strategic reorganization of the datasets to align with the database schema, ensuring an error-free import process. I refined my import techniques to accurately interpret and integrate the datasets, highlighting the importance of aligning personal preferences with optimal database practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skills Developed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During my internship, I cultivated a diverse skill set crucial for proficient database management and optimization. Key skills acquired include:\n",
    "\n",
    "* In-Depth Knowledge of Database Management: I gained a comprehensive understanding of common challenges faced in database management, enabling me to analyze and resolve issues related to data integrity, query performance, and schema design across MySQL, PostgreSQL, and SQL environments.\n",
    "\n",
    "* Hands-On Problem-Solving in MySQL, PostgreSQL, and SQL: I actively applied problem-solving skills to tackle challenges specific to MySQL, PostgreSQL, and SQL. This involved troubleshooting and resolving data formatting issues, optimizing queries, and managing data import/export procedures.\n",
    "\n",
    "* Analysis of Data Quality: I developed skills to assess and ensure data integrity, identify common data issues, and propose effective measures to maintain a consistent data environment.\n",
    "\n",
    "* Emphasis on Comprehensive Documentation: I acknowledged the significance of thorough documentation and best practices, enhancing my awareness of the importance of documenting database schemas and structures for future reference.\n",
    "\n",
    "These skills collectively form a robust foundation for effective database management and are pivotal for addressing real-world challenges in the dynamic field of information technology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Internship Experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My 15-day internship was a transformative experience, blending foundational learning with hands-on application in the database management domain. The first week immersed me in SQL, MySQL, and PostgreSQL fundamentals, preparing me for the subsequent assessment of the music store database. This evaluation included a detailed review of the database schema, careful analysis of data types and constraints, and strategic query optimization. Practical challenges, such as formatting data for import/export, provided invaluable problem-solving experiences. Collaborating directly with the Computer Science Department on-site enhanced communication, compensating for the absence of virtual tools. The internship concluded with a comprehensive report encapsulating my findings and reflections. I developed skills in database schema design, troubleshooting common issues, and effective communication. This experience not only applied theoretical knowledge but also instilled adaptability and real-world problem-solving skills, marking a significant milestone in my journey to becoming a proficient and versatile database professional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Achievements and Milestones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The internship yielded notable achievements and milestones that advanced my database management skills. Successfully evaluating and optimizing the music store database demonstrated my practical expertise. Overcoming challenges, such as data formatting nuances, underscored my adaptability and problem-solving capabilities. Collaborating on-site with the Computer Science Department fostered effective communication and enriched my overall learning experience. The comprehensive report I prepared serves as a tangible milestone, synthesizing theoretical knowledge into practical application. This experience stands as a testament to continuous learning and growth, highlighting achievements that contribute to a solid foundation for my future endeavors in the dynamic field of database management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This 15-day internship has been an invaluable journey characterized by immersive learning, practical application, and skill development. Engaging with real-world challenges in the context of a music store database provided a rich platform for honing my database management expertise. The experience underscored the importance of adaptability, problem-solving, and meticulous attention to detail in ensuring the efficiency and integrity of a database system. I am deeply grateful for the guidance, support, and collaborative spirit of the Computer Science Department team, which significantly contributed to the success of this endeavor. This internship has not only expanded my technical proficiency but has also instilled a profound appreciation for the intricacies of database management in a professional setting. As I conclude this internship, I carry forward a wealth of experiences and insights that will undoubtedly shape my future endeavors in the field of database management."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5829368,
     "sourceId": 9564994,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
